{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机森林(RandomForest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考资料:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://blog.csdn.net/qq547276542/article/details/78304454\n",
    "\n",
    "https://www.toutiao.com/a6455335677880959502/\n",
    "\n",
    "https://www.cnblogs.com/fionacai/p/5894142.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 概述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机森林属于集成学习（Ensemble Learning）中的bagging算法。在集成学习中，主要分为bagging算法和boosting算法。我们先看看这两种方法的特点和区别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Bagging（套袋法）**\n",
    "\n",
    "bagging的算法过程如下：\n",
    "\n",
    "* 从原始样本集中使用Bootstraping方法随机抽取n个训练样本，共进行k轮抽取，得到k个训练集。（k个训练集之间相互独立，元素可以有重复）\n",
    "* 对于k个训练集，我们训练k个模型（这k个模型可以根据具体问题而定，比如决策树，knn等）\n",
    "* 对于分类问题：由投票表决产生分类结果；对于回归问题：由k个模型预测结果的均值作为最后预测结果。（所有模型的重要性相同）\n",
    "\n",
    "** Boosting（提升法）**\n",
    "boosting的算法过程如下：\n",
    "\n",
    "* 对于训练集中的每个样本建立权值wi，表示对每个样本的关注度。当某个样本被误分类的概率很高时，需要加大对该样本的权值。\n",
    "* 进行迭代的过程中，每一步迭代都是一个弱分类器。我们需要用某种策略将其组合，作为最终模型。（例如AdaBoost给每个弱分类器一个权值，将其线性组合最为最终分类器。误差越小的弱分类器，权值越大）\n",
    "\n",
    "** Bagging，Boosting的主要区别 **\n",
    "\n",
    "* 样本选择上：Bagging采用的是Bootstrap随机有放回抽样；而Boosting每一轮的训练集是不变的，改变的只是每一个样本的权重。\n",
    "* 样本权重：Bagging使用的是均匀取样，每个样本权重相等；Boosting根据错误率调整样本权重，错误率越大的样本权重越大。\n",
    "* 预测函数：Bagging所有的预测函数的权重相等；Boosting中误差越小的预测函数其权重越大。\n",
    "* 并行计算：Bagging各个预测函数可以并行生成；Boosting各个预测函数必须按顺序迭代生成。\n",
    "\n",
    "下面是将决策树与这些算法框架进行结合所得到的新的算法：\n",
    "\n",
    "1）Bagging + 决策树 = 随机森林\n",
    "\n",
    "2）AdaBoost + 决策树 = 提升树\n",
    "\n",
    "3）Gradient Boosting + 决策树 = GBDT\n",
    "\n",
    "这块的资料可以参考:\n",
    "http://blog.csdn.net/code_lr/article/details/51446492\n",
    "\n",
    "http://blog.csdn.net/qq547276542/article/details/78304454"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树提供了一个简单、清晰的概念模型来理解迭代分类过程。然而，在实践中，单个决策树对于解决涉及大量变量和中等大小规模数据的现实问题时，并不十分有效。因此，我们需要重型武器——由一群“专家”组成的决策森林。\n",
    "\n",
    "这里的“专家”指的是什么呢？\n",
    "\n",
    "我们认为的“专家”们，每人脑子里有一棵决策树模型，然后将这样的很多个专家集合在一起，不那么严格地讲，我们就创建了一个随机森林。当我们期望决策结果好于单个决策树的时候，我们会想要一群“专家”来帮忙做决策。因此，我们需要一些方法来决定如何整理和排序“专家”们的“意见”。\n",
    "\n",
    "当然为了让决策意见更多样化,所以在构建这些专家时,使用随机样本,随机特征,这样构建的专家就会更具差异性,意见也就更丰富了 决策效果就更好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优缺点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 随机森林有许多优点：**\n",
    "\n",
    "* 具有极高的准确率\n",
    "* 随机性的引入，使得随机森林不容易过拟合\n",
    "* 随机性的引入，使得随机森林有很好的抗噪声能力\n",
    "* 能处理很高维度的数据，并且不用做特征选择\n",
    "* 既能处理离散型数据，也能处理连续型数据，数据集无需规范化\n",
    "* 训练速度快，可以得到变量重要性排序\n",
    "* 容易实现并行化\n",
    "\n",
    "** 随机森林的缺点：**\n",
    "\n",
    "* 当随机森林中的决策树个数很多时，训练时需要的空间和时间会较大\n",
    "* 随机森林模型还有许多不好解释的地方，有点算个黑盒模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 通过自助法(bootstrap)构建大小为n的一个训练集，即重复抽样选择n个训练样例\n",
    "* 对于刚才新得到的训练集，构建一棵决策树。在每个节点执行以下操作：\n",
    "* 通过不重复抽样选择d个特征\n",
    "* 利用上面的d个特征，选择某种度量分割节点\n",
    "* 重复步骤1和2，k次\n",
    "* 对于每一个测试样例，对k颗决策树的预测结果进行投票。票数最多的结果就是随机森林的预测结果。\n",
    "\n",
    "随机森林中构建决策树的做法和原始决策树的区别是，在每次分割节点时，不是从所有特征中选择而是在一个小特征集中选择特征。\n",
    "\n",
    "虽然随机森林模型的可解释性不如决策树，但是它的一大优点是受超参数的影响波动不是很大(译者注：几个主要参数还是需要好好调参的)。我们也不需要对随机森林进行剪枝因为集成模型的鲁棒性很强，不会过多受单棵决策树噪音的影响。\n",
    "\n",
    "\n",
    "在实际运用随机森林模型时，树的数目(k)需要好好调参。一般，k越大，随机森林的性能越好，当然计算成本也越高。\n",
    "\n",
    "样本大小n能够控制bias-variance平衡，如果n很大，我们就减小了随机性因此随机森林就容易过拟合。另一方面，如果n很小，虽然不会过拟合，但模型的性能会降低。大多数随机森林的实现，包括sklearn中的RandomForestClassifier，n的大小等于原始训练集的大小。\n",
    "\n",
    "\n",
    "在每一次分割时特征集的大小d，一个最起码的要求是要小于原始特征集大小，sklearn中的默认值，其中m是原始特征集大小，这是一个比较合理的数值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用鸢尾花数据集作为测试数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris=load_iris()\n",
    "X_train,X_test,y_train,y_test=train_test_split(iris.data,iris.target,test_size=0.3,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "载入随机森林模型 其中n_estimators表示树的个数  criterion表示分割特征的方式 默认gini 可选entropy     max_depth最大深度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest=RandomForestClassifier(n_estimators=10,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印混淆矩阵让预测结果更加直观"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16,  0,  0],\n",
       "       [ 0, 17,  1],\n",
       "       [ 0,  0, 11]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97777777777777775"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还可以通过随机森林做特征选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以通过随机森林中所有决策树得到**平均不纯度衰减**来度量特征的重要性,sklearn中就是训练后模型的 feature_importances_ 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.14334485,  0.0264803 ,  0.40058315,  0.42959169])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过不纯度衰减我们可以知道  \n",
    "* petal width 对于分类是最重要的特征  \n",
    "* sepal width特征对于分类是最不重要的 \n",
    "\n",
    "我们现在把最不重要的特征去掉 再重新训练看会不会影响结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5. ,  2. ,  3.5,  1. ],\n",
       "       [ 6.5,  3. ,  5.5,  1.8],\n",
       "       [ 6.7,  3.3,  5.7,  2.5],\n",
       "       [ 6. ,  2.2,  5. ,  1.5],\n",
       "       [ 6.7,  2.5,  5.8,  1.8]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2 = np.delete(X_train,1,axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5. ,  3.5,  1. ],\n",
       "       [ 6.5,  5.5,  1.8],\n",
       "       [ 6.7,  5.7,  2.5],\n",
       "       [ 6. ,  5. ,  1.5],\n",
       "       [ 6.7,  5.8,  1.8]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_2=np.delete(X_test,1,axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest_2=RandomForestClassifier(n_estimators=10,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_2.fit(X_train_2,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_2=forest_2.predict(X_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16,  0,  0],\n",
       "       [ 0, 17,  1],\n",
       "       [ 0,  0, 11]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,y_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97777777777777775"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_2.score(X_test_2,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.24582337,  0.46004092,  0.29413571])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_2.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从分类结果可以看出去掉  sepal width对分类没有影响 \n",
    "\n",
    "但有点不理解的是 上一个模型中最重要的特征 现在这个模型中确变成第二重要了 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
